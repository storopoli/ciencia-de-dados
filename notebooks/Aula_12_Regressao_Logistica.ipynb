{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/storopoli/ciencia-de-dados/master?filepath=notebooks%2FAula_12_Regressao_Logistica.ipynb)\n",
    "<br>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/storopoli/ciencia-de-dados/blob/master/notebooks/Aula_12_Regressao_Logistica.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regressão Logística\n",
    "\n",
    "**Objetivos**: Aprender o que é Regressão Logística e Regressão Softmax usando a biblioteca `Scikit-Learn`. Introduzir o aluno aos problemas de classificação de aprendizagem de máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defininição - Regressão Logística\n",
    "\n",
    "> Uma regressão logística se comporta exatamente como um modelo linear: faz uma predição simplesmente computando uma soma ponderada dos atributos (*features*), mais uma constante chamada viés (*bias*), também chamado de constante (*intercept*). Porém ao invés de retornar um valor contínuo, como a regressão linear, retorna a função logística desse valor.\n",
    "\n",
    "$$\\operatorname{Logística}(x) = \\frac{1}{1 + e^{(-x)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "sig = 1 / (1 + np.exp(-x))\n",
    "plt.plot(x, sig, \"b-\", linewidth=2, label=r\"$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend(loc=\"upper left\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ \\operatorname{Linear} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n$$\n",
    "\n",
    "$\\operatorname{Linear}$ - regressão linear\n",
    "\n",
    "$\\theta$ - parâmetro do modelo\n",
    "\n",
    "$n$ - número de atributos (*features*)\n",
    "\n",
    "$x_i$ - o valor do *inésimo* atributo (*feature*)\n",
    "\n",
    "$ \\hat{p} = \\sigma(\\operatorname{Linear}) = \\frac{1}{1 + e^{-\\operatorname{Linear}}}$\n",
    "\n",
    "$\\hat{p}$ - probabilidade prevista da observação ser 1\n",
    "\n",
    "$\\hat{y}=\\left\\{\\begin{array}{ll} 0 & \\text { se } \\hat{p} < 0.5 \\\\ 1 & \\text { se } \\hat{p} \\geq 0.5 \\end{array}\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Exemplo\n",
    "\n",
    "$\\mathrm{Previsão~de~Morte} = \\sigma \\big(-10 + 10\\times \\mathrm{cancer} + 12 \\times \\mathrm{diabetes} + 8 \\times \\mathrm{obesidade} \\big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Métricas de Desempenho de uma Regressão Logística\n",
    "\n",
    "### MSE (?)\n",
    "$$MSE = \\frac{1}{m}\\Sigma_{i=1}^{m}{(\\hat{y}_i - y_i)^2}$$\n",
    "Nossa predição agora é não-linear (devido à transformação logística). Se nós elevarmos essa predição ao quadrado (como fazemos em MSE) resulta em uma função não convexa e com muitos local minima, portanto, inviabilizando o Método do Gradiente Descendente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### *Log Loss* \n",
    "\n",
    "Se divide em duas métricas de desempenho\n",
    "\n",
    "$$Log~Loss=\\left\\{\\begin{array}{ll} -\\log(\\hat{p}) & \\text { se } y = 1 \\\\ -\\log(1 - \\hat{p}) & \\text { se } y = 0 \\end{array}\\right.$$\n",
    "\n",
    "Faz sentido porque:\n",
    "\n",
    "* $- \\log(\\hat{p})$ se torna grande quando $\\hat{p}$ se aproxima de 0 - Erro vai ser grande quando o modelo prevê $\\hat{p} \\approx 0$ mas $y = 1$\n",
    "* $- \\log(1 - \\hat{p})$ se torna grande quando $1- \\hat{p}$ se aproxima de 0 - Erro vai ser grande quando o modelo prevê $\\hat{p} \\approx 1$ mas $y = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2, sharey=True)\n",
    "\n",
    "x = np.linspace(0.001, 0.999, 100)  # evitar erros pq log(0) é inf e log(1) é 0\n",
    "ax[0].plot(x, -np.log(x))\n",
    "ax[0].set_title('Se $y=1$')\n",
    "ax[0].set_xlabel('$\\hat{p}$')\n",
    "ax[0].set_ylabel('Erro')\n",
    "\n",
    "ax[1].plot(x, -np.log(1-x))\n",
    "ax[1].set_title('Se $y=0$')\n",
    "ax[1].set_xlabel('$\\hat{p}$')\n",
    "ax[1].set_ylabel('Erro')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Para todo o dataset\n",
    "\n",
    "$$Log~Loss = \\frac{1}{m} \\sum_{i=1}^{m} \\big[y^{(i)} \\log (\\hat{p}^{(i)}) + (1+y^{(i)}) \\log (1-\\hat{p}^{(i)}) \\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exemplo com o Dataset Titanic\n",
    "\n",
    "Contém 891 passageiros reais do Titanic que afundou em 15/04/1912 matando 1502 de 2224 passageiros e tripulação que estavam a bordo.\n",
    "\n",
    "* `survived`: *dummy* `0` ou `1` \n",
    "* `pclass`: Classe do Passageiro\n",
    "    - `1`: Primeira Classe\n",
    "    - `2`: Segunda Classe\n",
    "    - `3`: Terceira Classe\n",
    "* `sex`: Sexo `male` ou `female`\n",
    "* `age`: Idade\n",
    "* `sibsp`: Número de Irmãos (*Siblings*) e Esposas (*spouse*) a bordo\n",
    "* `parch`: Número de pais/filhos a bordo\n",
    "* `fare`: Valor pago pela passagem em libras\n",
    "* `embarked`: Porto que embarcou\n",
    "    - `C`: Cherbourg\n",
    "    - `Q`: Queenstown\n",
    "    - `S`: Southampton)\n",
    "* `class`: Mesmo que `pclass` só que em texto\n",
    "* `adult_male`: *dummy* para `age > 16` e `sex == 'male'`\n",
    "* `deck`: Qual deck a cabine do passageiro se situava\n",
    "* `alive`: Mesmo que survived só que com `yes` ou `no`\n",
    "* `alone`: *dummy* para se viajava sozinho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/titanic.png\" alt=\"titanic\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "feature_names = ['pclass', 'female', 'age', 'fare']\n",
    "titanic['female'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
    "titanic.dropna(subset=feature_names, inplace=True)  #891 para 714\n",
    "\n",
    "X = titanic[feature_names].to_numpy()\n",
    "y = titanic['survived'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X  # Numpy Array apenas com números"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "y  # Numpy Array 1-D com 0 ou 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print('Nomes dos Atributos: ', feature_names, '\\n')\n",
    "print('Tamanho de X: ', X.shape, '\\n')\n",
    "print('Tamanho de y: ', y.shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quebrando dataset em `train` e `test`\n",
    "\n",
    "Usar a função do Scikit-Learn [`sklearn.model_selection.train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "#### Argumentos:\n",
    "\n",
    "* matriz a ser dividida - `X` ou `y`\n",
    "* `test_size` - `float` ou `int` do tamanho do dataset de teste (padrão $0.25$)\n",
    "* `train_size` - padrão `1 - test_size`\n",
    "* `random_state` - `int` - seed do gerador de número randômicos (replicabilidade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print('Tamanho de X_train: ', X_train.shape, '\\n')\n",
    "print('Tamanho de X_test: ', X_test.shape, '\\n')\n",
    "print('Tamanho de y_train: ', y_train.shape, '\\n')\n",
    "print('Tamanho de y_test: ', y_test.shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regressão Logística\n",
    "Usar a função do Scikit-Learn [`sklearn.linear_model.SGDClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)\n",
    "\n",
    "#### Argumentos:\n",
    "* `loss` - `str`\n",
    "    * Log Loss - `'log'`\n",
    "* `max_iter` - `int` - Número máximo de iterações do *Gradient Descent*\n",
    "* `tol` - Tolerância - Critério de parada de treino\n",
    "* `random_state` - `int` - seed do gerador de número randômicos (replicabilidade)\n",
    "* `eta0` - `float` - Taxa de aprendizagem inicial\n",
    "    * padrão `0.01`\n",
    "* `learning_rate` - `str` - Taxa de aprendizagem\n",
    "    * Constante - `'constant'`\n",
    "    * Adapatativa - `'adaptive'`\n",
    "* `n_iter_no_change` - `int` - Somente se usar Taxa de Aprendizagem Adaptativa\n",
    "\n",
    "#### Retorna:\n",
    "* Objeto `estimator` do Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(loss='log', learning_rate='constant', max_iter=10,\n",
    "                   eta0=0.0001, verbose=1, tol=None, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Classe `Estimators`\n",
    "\n",
    "* `.fit()` - Treina o Modelo\n",
    "    * `X`\n",
    "    * `y`\n",
    "* `.predict()` - Gera predições do modelo\n",
    "    * `X`\n",
    "* `.coef_` - Retorna os coeficientes do modelo ($\\theta_i$)\n",
    "* `.intercept_` - Retorna o viés/constante (*bias/intercept*) do modelo ($\\theta_0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Coeficientes do modelo\n",
    "for feature, coef in zip(feature_names, clf.coef_[0].tolist()):\n",
    "    print(f\"{feature}: {round(coef,3)}\")\n",
    "\n",
    "# Constante do modelo\n",
    "print(f\"Constante: {clf.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Acurácia do Modelo\n",
    "Usar a função do Scikit-Learn [`sklearn.metrics.accuracy_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "\n",
    "Retorna um score de acurácia `float` entre $0$ e $1$\n",
    "\n",
    "#### Argumentos\n",
    "* `y_true`: Classes Verdadeiras\n",
    "    * 2 classes: vetor (1-D)\n",
    "    * Mais que 2 classes: matriz (2-D)\n",
    "* `y_pred`: Classes Previstas pelo Modelo\n",
    "    * 2 classes: vetor (1-D)\n",
    "    * Mais que 2 classes: matriz (2-D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_train_true = y_train\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_true = y_test\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(f\"Acurácia de Treino: {round(accuracy_score(y_train_true, y_train_pred), 2)}\")\n",
    "print('\\n ---------------------------\\n')\n",
    "print(f\"Acurácia de Teste: {round(accuracy_score(y_test_true, y_test_pred), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Expandindo para $k$ classes\n",
    "\n",
    "Podemos usar Regressão Logística para mais de uma duas classes ($k > 2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Função Softmax\n",
    "\n",
    "Dado um vetor $\\mathbf{x}$, a função Softmax computa um score $s_k(\\mathbf{x})$ para cada classe $k$, então estima a probabilidade de cada classe.\n",
    "\n",
    "$$\\hat{p}_k = \\sigma(\\mathbf{s}(\\mathbf{x}))_k = \\frac{e^{s_k(\\mathbf{x})}}{\\sum_{j=1}^K e^{s_j(\\mathbf{x})}}$$\n",
    "\n",
    "* $K$ - número de classes\n",
    "* $\\mathbf{s}(\\mathbf{x})$ - vetor contendo todos os scores de cada classe de uma observação $x$\n",
    "* $\\sigma(\\mathbf{s}(\\mathbf{x}))_k$ - a probabilidade estimada ($\\hat{p}_k$) que a obsevação $x$ pertence a classe $k$, dado os scores de cada classe para $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regressão Softmax\n",
    "\n",
    "Que nem a Regressão Logística, a Regressão Softmax prevê a classe com maior probabilidade estimada (que é simplesmente a classe com o maior score) \n",
    "\n",
    "$$\\hat{y} = \\max_k \\sigma(\\mathbf{s}(\\mathbf{x}))_k = \\max_k s_k(\\mathbf{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regressão Softmax (Regressão Logística Multinomial)\n",
    "Usar a função do Scikit-Learn [`sklearn.linear_model.Logistic Regression()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "Não possui opções de controle da taxa de aprendizagem do Método do Gradiente Descendente\n",
    "\n",
    "#### Argumentos:\n",
    "* `multi_class` - `str`\n",
    "    * `'ovr'` - Binário ($k=2$)\n",
    "    * `'multinomial'` - Multiclasses ($k > 2$)\n",
    "    * `'auto'` - se baseia nas dimensões da `array` `y` que é passada no `.fit()`\n",
    "* `max_iter` - `int` - Número máximo de iterações do *Gradient Descent*\n",
    "* `tol` - Tolerância - Critério de parada de treino\n",
    "* `random_state` - `int` - seed do gerador de número randômicos (replicabilidade)\n",
    "\n",
    "#### Retorna:\n",
    "* Objeto `estimator` do Scikit-Learn\n",
    "\n",
    "#### O que muda da Regressão Logística?\n",
    "Agora o meu $y$ é uma matriz (2-D) e não um vetor (1-D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Métrica de Desempenho de uma Regressão Softmax\n",
    "\n",
    "**Cross Entropy**: Estender a Log Loss para mais que duas classes\n",
    "\n",
    "$$Cross~Entropy = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^K y_k^{(i)} \\log (\\hat{p}_k^{(i)})$$\n",
    "\n",
    "* $y_k^{(i)}$ é a probabilidade alvo que a observação $i$ pertence à classe $k$. De maneira geral, ou é $1$ ou $0$, dependendo de qual classe $i$ pertence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![mapa-conceitual](https://github.com/storopoli/ciencia-de-dados/raw/master/Mapas%20Conceituais/13%20-%20Regressao%20Logistica.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Atividade - Regressão com o dataset [Iris](https://scikit-learn.org/stable/datasets/index.html#iris-dataset)\n",
    "\n",
    "Edgar Anderson coletou os dados para quantificar a variação morfológica das flores de íris de três espécies relacionadas.\n",
    "\n",
    "O conjunto de dados consiste em 50 amostras de cada uma das três espécies de Iris (Setosa, Virginica e Versicolor). Quatro características foram medidas em cada amostra (cm):\n",
    "\n",
    "* $N = 150$\n",
    "* Atributos: 4\n",
    "    * `sepal length (cm)` - Cumprimento da Sépala\n",
    "    * `sepal width (cm)` - Largura da Sépala\n",
    "    * `petal length (cm)` - Cumprimento da Pétala\n",
    "    * `petal width (cm)` - Largua da Sépala\n",
    "* Variável dependente: Tipo de espécie de Iris\n",
    "    * `0` - Setosa\n",
    "    * `1` - Virginica\n",
    "    * `2` - Versicolor \n",
    "\n",
    "* Achar a acurácia do modelo e os respectivos coeficientes dos atributos ($\\theta_i$) e viés/constante ($\\theta_0$)\n",
    "\n",
    ">Obs: usar `test_size = 0.25` e `random_state = 123`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://thegoodpython.com/assets/images/iris-species.png\" alt=\"iris-sepals-petals\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print('Nomes dos Atributos: ', iris['feature_names'], '\\n')\n",
    "print('Tamanho de X: ', X.shape, '\\n')\n",
    "print('Tamanho de y: ', y.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print('Tamanho de X_train: ', X_train.shape, '\\n')\n",
    "print('Tamanho de X_test: ', X_test.shape, '\\n')\n",
    "print('Tamanho de y_train: ', y_train.shape, '\\n')\n",
    "print('Tamanho de y_test: ', y_test.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bit4345593c936d4190a0f0d35ea30c0e1d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
