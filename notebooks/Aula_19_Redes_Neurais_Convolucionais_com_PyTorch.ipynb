{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/storopoli/ciencia-de-dados/main?filepath=notebooks%2FAula_19_Redes_Neurais_Convolucionais_com_PyTorch.ipynb)\n",
    "<br>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/storopoli/ciencia-de-dados/blob/main/notebooks/Aula_19_Redes_Neurais_Convolucionais_com_PyTorch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Redes Neurais Convolucionais com PyTorch\n",
    "\n",
    "**Objetivos**: Aprender Redes Neurais Convolucionais (_Convolution Neural Networks_ - CNN) usando a biblioteca `PyTorch`.\n",
    "\n",
    "> Observação: Algumas imagens foram adaptadas de [Adventures in Machine Learning](https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/) (Licença CC-BY-SA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## PyTorch\n",
    "\n",
    "[**PyTorch**](https://www.pytorch.org/) é uma biblioteca de código aberto para aprendizado de máquina aplicável a uma ampla variedade de tarefas. Foi criada pelo **Facebook** em 2016 é a principal biblioteca para criação e treinamento de redes neurais artificiais. A API toda é escrita em Python mas é executada em C++ na CPU ou em CUDA/ROCM na GPU.\n",
    "\n",
    "No momento que eu escrevo esse tutorial (Abril de 2021), PyTorch está superando o TensorFlow (Google) em desempenho e adoção de uso. Isso acontece tanto na [academia](http://horace.io/pytorch-vs-tensorflow/) (mensurado pela adoção de artigos científicos nos principais encontros científicos de Aprendizagem Profunda e Aprendizagem de Máquina) quanto na [indústria](https://www.infoworld.com/article/3597904/why-enterprises-are-turning-from-tensorflow-to-pytorch.html) (mensurado pela adoção de grandes e renomadas empresas de tecnologia).\n",
    "\n",
    "### Atualização (Junho 2022):\n",
    "\n",
    "TensorFlow está morto, Vida Longa ao PyTorch: [Google lost the battle for machine learning to Meta, insiders say. Now it's betting the future of its own products on a new internal AI project.](https://www.businessinsider.com/facebook-pytorch-beat-google-tensorflow-jax-meta-ai-2022-6).\n",
    "\n",
    "> Now, under the shadow of PyTorch, Google has been quietly building out a machine learning framework, called JAX (at one point an acronym for \"Just After eXecution,\" but officially no longer stands for anything), that many see as the successor to TensorFlow.\n",
    "\n",
    "Meus comentários: JAX é mais um backend de _autodiff_ do que uma biblioteca de redes neurais. \n",
    "Ou seja tem um uso muito mais amplo que PyTorch.\n",
    "Por exemplo, como um amostrador Monte Carlo de correntes Markov (_Markov Chain Monte Carlo_ - MCMC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Imagens e Filtros\n",
    "\n",
    "Todo dado estruturado e não-esturutado é representado como matrizes no computador.\n",
    "Veja o caso de imagens.\n",
    "Elas são literalmente matrizes multidimensionais.\n",
    "\n",
    "* Uma imagem colorida é uma Array de dimensão $X \\times Y \\times 3$:\n",
    "    * $X$ = quantidade de pixels no eixo horizontal\n",
    "    * $Y$ = quantidade de pixels no eixo vertical\n",
    "    * $3$ = são 3 canais de cores - **R**ed, **G**reen e **B**lue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/images_as_matrix.png\" alt=\"Row vs Cols\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Convoluções\n",
    "\n",
    "Em matemática **[convolução](https://pt.wikipedia.org/wiki/Convolu%C3%A7%C3%A3o) é um operador linear** que, a partir de **duas funções dadas**, **resulta numa terceira que mede a soma do produto dessas funções ao longo da região** subentendida pela superposição delas em função do deslocamento existente entre elas.\n",
    "\n",
    "A notação para a convolução de $f$ e $g$ é $f*g$. Ela é definida como a integral do produto de uma das funções por uma cópia deslocada e invertida da outra; a função resultante $h$ depende do valor do deslocamento. Se $x$ for a variável independente e $u$, o deslocamento, a fórmula pode ser escrita como:\n",
    "\n",
    "$$(f * g) (x) = h(x) = \\int_{-\\infty}^{\\infty} f(u) \\cdot g(x-u) du$$\n",
    "\n",
    "Existe ainda uma definição de **convolução para funções de domínio discreto**, dada por\n",
    "\n",
    "$$(f * g) (k) = h(k)= \\sum_{j=0}^{k} f(j) \\cdot g(k-j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/convolution.gif\" alt=\"Row vs Cols\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Inclusive, convoluções, onde a \"máquina\" aprende os filtros (os números que vão dentro da matriz), é a base das redes neurais que detectam objetos!\n",
    "\n",
    "<img src=\"images/deeplearning_convolutions.gif\" alt=\"Deep Learning Convolutions\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## O que é uma Rede Neural Artificial?\n",
    "\n",
    "Redes neurais artificiais (RNAs) são modelos computacionais inspirados pelo sistema nervoso central (em particular o cérebro) que são capazes de realizar o aprendizado de máquina bem como o reconhecimento de padrões. Redes neurais artificiais geralmente são apresentadas como sistemas de \"neurônios interconectados, que podem computar valores de entradas\", simulando o comportamento de redes neurais biológicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/redes_neurais.jpeg\" alt=\"redes neurais\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Como a rede neural aprende?\n",
    "\n",
    "Em cada neurônio há uma função de ativação (*activation function*) que processa uma combinação linear entre inputs e pesos sinápticos, gerando assim um sinal de saída.\n",
    "\n",
    "A informação flui da *input layer* para as *hidden layers* e por fim para a *output layer*. Nesse fluxo os inputs de dados da *input layer* são alimentados para os neurônios das *hidden layers* que por fim alimentam o neurônio final da *output layer*.\n",
    "\n",
    "A primeira passada de informação (propagação) pela rede é geralmente feita com parâmetros aleatórios para as funções de ativação dos neurônios.\n",
    "\n",
    "Ao realizar a propagação, chamada de *feed forward*, temos sinais de saídas nos neurônios da output layer. \n",
    "\n",
    "No fim da propagação, a função custo (uma métrica de erro) é calculada e o modelo então ajusta os parâmetros dos neurônios na direção de um menor custo (por meio do gradiente - derivada multivariada).\n",
    "\n",
    "Assim uma nova propagação é gerada e a numa nova função custo e calculada. Assim como é realizado a atualização dos parâmetros dos neurônios.\n",
    "\n",
    "O nome desse algoritmo é **Retro-propagação** (*Backpropagation*). E cada vez que ele é executado denomina-se como época (*epoch*). E quandos as épocas estabelecidas se encerram, a rede neural encerra o seu treinamento/aprendizagem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/backpropagation.gif\" alt=\"backpropagation\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Por que Redes Neurais Convolucionais?\n",
    "As Redes Neurais Convolucionais têm a capacidade de **extrair automaticamente características dos padrões a serem aprendidos** (por meio das camadas de convolução), tarefa que necessariamente tem que ser implementada separadamente quando se emprega uma Rede Neural padrão (\"vanilla\", _**M**ulti**l**ayer **P**erceptron_ - MLP) ou um outro classificador convencional (por exemplo, _**S**upport **V**ector **M**achine_ - SVM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### O que muda numa Rede Neural Convolucional?\n",
    "\n",
    "**Nada**.\n",
    "\n",
    "Apenas que a rede neural possui um maior número de parâmetros pois precisa aprender todos os parâmetros dos filtros e convoluções que ocorrem em algumas de suas camadas internas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Camadas Necessárias para Convolução\n",
    "\n",
    "As redes neurais convolucionais se distinguem de outras redes neurais por seu desempenho superior com dados de imagem, voz ou áudio. Elas têm três tipos principais de camadas, que são:\n",
    "\n",
    "* Camada Convolucional\n",
    "* Camada de _Pooling_\n",
    "* Camada de Neurônios Totalmente Conectados (também chamado de _Fully Connected_ ou _Dense_) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Camadas de Convolução (_Convolution Layers_)\n",
    "\n",
    "Isto é uma camada para uma \"janela\" ou \"filtro\" deslizante que passamos pela imagem.\n",
    "Essa \"janela\" aplica um certo peso aos neurônios (pixels) adjacentes de uma certa vizinhança de pixels.\n",
    "\n",
    "Filtro $2 \\times 2$ com todos os pesos igual a $0.5$:\n",
    "\n",
    "<img src=\"images/moving-filter.webp\" alt=\"filtro convolução 2x2\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\begin{align}\n",
    " out_1 &= 0.5 in_1 + 0.5 in_2 + 0.5 in_6 + 0.5 in_7 \\\\\n",
    " &= 0.5 \\times 2.0 + 0.5 \\times 3.0 + 0.5 \\times 2.0 + 0.5 \\times 1.5  \\\\\n",
    " &= 4.25 \\\\\n",
    " out_2 &= 0.5 in_2 + 0.5 in_3 + 0.5 in_7 + 0.5 in_8 \\\\\n",
    " &= 0.5 \\times 3.0 + 0.5 \\times 0.0 + 0.5 \\times 1.5 + 0.5 \\times 0.5  \\\\\n",
    " &= 2.5 \\\\\n",
    " \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Na rede neural este filtro funciona assim quando compararmos com relação a camada anterior (_inputs_):\n",
    "\n",
    "<img src=\"images/moving-filter-node-diagram.webp\" alt=\"filtro convolução 2x2\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Camadas de _Pooling_\n",
    "\n",
    "Uma outra técnica de \"janela\" ou \"filtro\" deslizante, mas ao invés de aplicar pesos, aplica uma função estatística sobre os conteúdos da sua \"janela\".\n",
    "\n",
    "A função estatística mais comum é o máximo `max()`.\n",
    "Essa é chamada de _max pooling_.\n",
    "\n",
    "##### Duas vantagens:\n",
    "\n",
    "- Reduz o número de parâmetros do modelo por um processo chamado _down-sampling_\n",
    "- Faz com que a detecção de atributos seja mais robusta à mudanças na orientação do objeto e mudanças de escalas\n",
    "\n",
    "_Max pooling_ com _padding_:\n",
    "\n",
    "<img src=\"images/max-pooling.webp\" alt=\"Max Pooling\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Juntanto todas as camadas\n",
    "\n",
    "<img src=\"images/typical_cnn.webp\" alt=\"Rede Neural Convolucional Típica\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Estruturação dos módulos de PyTorch\n",
    "\n",
    "```python\n",
    "import torch\n",
    "```\n",
    "* [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html) - Tensores (arrays N-D)\n",
    "* [`torch.nn`](https://pytorch.org/docs/stable/nn.html) - Redes Neurais (_**N**eural **N**etworks_)\n",
    "* [`torch.optim`](https://pytorch.org/docs/stable/optim.html) - Otimização (_**Optim**ization_)\n",
    "* [`torch.data`](https://pytorch.org/docs/stable/data.html) - *Datasets* e Ferramentas de Streaming de Dados\n",
    "* [`torch.autograd`](https://pytorch.org/docs/stable/autograd.html) - Diferenciação Automática (_**Auto**matic Differentiation_)\n",
    "* [`torch.vision`](https://pytorch.org/docs/stable/torchvision/index.html) - Ferramentas de Manipulação de Imagens e Visão Computacional\n",
    "* [`torch.audio`](https://pytorch.org/audio/stable/index.html) - Ferramentas de Manipulação de Áudio\n",
    "* [`torch.jit`](https://pytorch.org/docs/stable/jit.html) - Compilação _**j**ust-**i**n-**t**ime_ de modelos PyTorch em binários\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `torch.Tensor`\n",
    "\n",
    "* `NumPy` - `np.ndarray`\n",
    "* `pandas` - `pd.Series` e `pd.DataFrame`\n",
    "* `PyTorch` - `torch.Tensor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exemplo com o [MNIST](https://en.wikipedia.org/wiki/MNIST_database)\n",
    "\n",
    "O dataset **MNIST** (_**M**odified **N**ational **I**nstitute of **S**tandards and **T**echnology_) é um clássico de Deep Learning. São imagens preto e branco de dígitos de 0 a 9 de formato 28x28 pixels (784 pixels)\n",
    "\n",
    "Contém 60.000 imagens de treino e 10.000 de teste e é um padrão de benchmark de modelos (acho que \"foi\", não é tão usado assim mais)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/MNIST.png\" alt=\"MNIST\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "O pacote `torchvision` do `PyTorch` tem integrado já o dataset MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# MNIST dataset\n",
    "root_path = '/home/storopoli/Downloads' # mude isso no Colab se necessário\n",
    "\n",
    "# Pequena transformação para tensores e normalizando o tamanho\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# Train/Test Datasets\n",
    "train_dataset = torchvision.datasets.MNIST(root=root_path, train=True, transform=trans, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root=root_path, train=False, transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: /home/storopoli/Downloads\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: /home/storopoli/Downloads\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Para controlar como que os dados são inseridos no modelo e, logo, o Batch Size é preciso implementar um [`torch.utils.data.DataLoader()`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
    "\n",
    "Argumentos do `DataLoader()`: \n",
    "\n",
    "* **`dataset`**: um `Dataset` PyTorch\n",
    "    * tem [vários tipos](https://pytorch.org/docs/stable/data.html)\n",
    "    * no nosso exemplo vou usar um simples [`TensorDataset`](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset) que é um wrapper de `np.ndarray` e `pd.Series` para `torch.Tensor`)\n",
    "* **batch_size**: `int` - tamanho do Batch Size, padrão é 1\n",
    "* **shuffle**: `bool` - se vai embaralhar os dados antes de enviar em batches ao modelo, padrão é `False`. Recomendo usar `shuffle=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### [Camadas de Convolução (_Convolution Layers_)](https://pytorch.org/docs/stable/nn.html#convolution-layers)\n",
    "\n",
    "* [`nn.Conv1d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv3d): convolução 1-D.\n",
    "* [`nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html): convolução 2-D.\n",
    "* [`nn.Conv3d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html): convolução 3-D.\n",
    "\n",
    "#### [Camadas de _Pooling_](https://pytorch.org/docs/stable/nn.html#pooling-layers)\n",
    "\n",
    "* [`nn.MaxPool1d`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.htm): 1-D _max pooling_.\n",
    "* [`nn.MaxPool2d`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.htm): 2-D _max pooling_.\n",
    "* [`nn.MaxPool3d`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.htm): 3-D _max pooling_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Qual a rede neural convolucional que vamos criar para o MNIST?\n",
    "\n",
    "<img src=\"images/MNIST-CNN.webp\" alt=\"MNIST CNN\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Como construir sua rede neural convolucional no PyTorch\n",
    "\n",
    "Construir redes neurais com o **PyTorch** é fácil.\n",
    "\n",
    "Temos que criar uma Rede Neural a partir de uma classe [`nn.Module()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=module#torch.nn.Module) e criar um construtor com o método `__init__()` e implementar todas as layers e propagações desejadas com o método `forward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=1000, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): Linear(in_features=1000, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(7 * 7 * 64, 1000),\n",
    "            nn.ReLU())\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Instancia o Model()\n",
    "model = ConvNet()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Quantos paramêtros temos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3199106"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Como treinar sua rede neural no PyTorch\n",
    "\n",
    "Uma vez especificado e instanciado o modelo, podemos manipulá-lo de maneira dinâmica.\n",
    "Escolhemos a função custo (`loss_fn`) como `nn.CrossEntropyLoss()` e também a taxa de aprendizagem $\\eta$ em `1e-6` e a quantidade de épocas a serem treinadas (`epochs`):\n",
    "\n",
    "```python\n",
    "model = Sua_rede_neural()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-6\n",
    "epochs = 100\n",
    "\n",
    "# Instânciar o Otimizador Adam\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# For-loop de treino\n",
    "for epoch in range(epochs): # para cada época\n",
    "    for (images, labels) in train_loader: # para cada batch\n",
    "        # Gera a propagação (feed forward)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calcula a função-custo\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Retro-propagação (Backprop) e a otimização com Adam\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "# Hiperparâmetros\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "epochs = 6\n",
    "\n",
    "# Instânciar o Otimizador Adam\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### OBSERVAÇÃO\n",
    "\n",
    "Esta rede neural possui muitos parâmetros.\n",
    "Aproximadamente 3 milhões e 200 mil paramêtros!\n",
    "Não tente rodar isso sem ser usando uma GPU!\n",
    "O Google Colab te dá uma GPU de graça.\n",
    "Clique no botão abaixo para abrir este notebook no Google Colab.\n",
    "Não esqueça de trocar sua `Runtime` de `None` para `GPU`.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/storopoli/ciencia-de-dados/blob/main/notebooks/Aula_19_Redes_Neurais_Convolucionais_com_PyTorch.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Isto tem que retornar True\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3070 Ti'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sua GPU\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época [1/6], Step [100/1875], Custo: 0.255, Acurácia: 96.875\n",
      "Época [1/6], Step [200/1875], Custo: 0.178, Acurácia: 93.75\n",
      "Época [1/6], Step [300/1875], Custo: 0.062, Acurácia: 100.0\n",
      "Época [1/6], Step [400/1875], Custo: 0.032, Acurácia: 96.875\n",
      "Época [1/6], Step [500/1875], Custo: 0.044, Acurácia: 100.0\n",
      "Época [1/6], Step [600/1875], Custo: 0.005, Acurácia: 100.0\n",
      "Época [1/6], Step [700/1875], Custo: 0.118, Acurácia: 96.875\n",
      "Época [1/6], Step [800/1875], Custo: 0.017, Acurácia: 100.0\n",
      "Época [1/6], Step [900/1875], Custo: 0.104, Acurácia: 96.875\n",
      "Época [1/6], Step [1000/1875], Custo: 0.034, Acurácia: 100.0\n",
      "Época [1/6], Step [1100/1875], Custo: 0.013, Acurácia: 100.0\n",
      "Época [1/6], Step [1200/1875], Custo: 0.14, Acurácia: 93.75\n",
      "Época [1/6], Step [1300/1875], Custo: 0.033, Acurácia: 96.875\n",
      "Época [1/6], Step [1400/1875], Custo: 0.001, Acurácia: 100.0\n",
      "Época [1/6], Step [1500/1875], Custo: 0.001, Acurácia: 100.0\n",
      "Época [1/6], Step [1600/1875], Custo: 0.017, Acurácia: 100.0\n",
      "Época [1/6], Step [1700/1875], Custo: 0.014, Acurácia: 100.0\n",
      "Época [1/6], Step [1800/1875], Custo: 0.011, Acurácia: 100.0\n",
      "Época [2/6], Step [100/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [2/6], Step [200/1875], Custo: 0.009, Acurácia: 100.0\n",
      "Época [2/6], Step [300/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [2/6], Step [400/1875], Custo: 0.032, Acurácia: 96.875\n",
      "Época [2/6], Step [500/1875], Custo: 0.005, Acurácia: 100.0\n",
      "Época [2/6], Step [600/1875], Custo: 0.095, Acurácia: 93.75\n",
      "Época [2/6], Step [700/1875], Custo: 0.002, Acurácia: 100.0\n",
      "Época [2/6], Step [800/1875], Custo: 0.025, Acurácia: 100.0\n",
      "Época [2/6], Step [900/1875], Custo: 0.12, Acurácia: 96.875\n",
      "Época [2/6], Step [1000/1875], Custo: 0.002, Acurácia: 100.0\n",
      "Época [2/6], Step [1100/1875], Custo: 0.002, Acurácia: 100.0\n",
      "Época [2/6], Step [1200/1875], Custo: 0.011, Acurácia: 100.0\n",
      "Época [2/6], Step [1300/1875], Custo: 0.002, Acurácia: 100.0\n",
      "Época [2/6], Step [1400/1875], Custo: 0.012, Acurácia: 100.0\n",
      "Época [2/6], Step [1500/1875], Custo: 0.16, Acurácia: 96.875\n",
      "Época [2/6], Step [1600/1875], Custo: 0.004, Acurácia: 100.0\n",
      "Época [2/6], Step [1700/1875], Custo: 0.048, Acurácia: 96.875\n",
      "Época [2/6], Step [1800/1875], Custo: 0.005, Acurácia: 100.0\n",
      "Época [3/6], Step [100/1875], Custo: 0.005, Acurácia: 100.0\n",
      "Época [3/6], Step [200/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [3/6], Step [300/1875], Custo: 0.036, Acurácia: 100.0\n",
      "Época [3/6], Step [400/1875], Custo: 0.003, Acurácia: 100.0\n",
      "Época [3/6], Step [500/1875], Custo: 0.002, Acurácia: 100.0\n",
      "Época [3/6], Step [600/1875], Custo: 0.015, Acurácia: 100.0\n",
      "Época [3/6], Step [700/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [3/6], Step [800/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [3/6], Step [900/1875], Custo: 0.003, Acurácia: 100.0\n",
      "Época [3/6], Step [1000/1875], Custo: 0.03, Acurácia: 100.0\n",
      "Época [3/6], Step [1100/1875], Custo: 0.001, Acurácia: 100.0\n",
      "Época [3/6], Step [1200/1875], Custo: 0.004, Acurácia: 100.0\n",
      "Época [3/6], Step [1300/1875], Custo: 0.003, Acurácia: 100.0\n",
      "Época [3/6], Step [1400/1875], Custo: 0.002, Acurácia: 100.0\n",
      "Época [3/6], Step [1500/1875], Custo: 0.006, Acurácia: 100.0\n",
      "Época [3/6], Step [1600/1875], Custo: 0.001, Acurácia: 100.0\n",
      "Época [3/6], Step [1700/1875], Custo: 0.027, Acurácia: 100.0\n",
      "Época [3/6], Step [1800/1875], Custo: 0.124, Acurácia: 96.875\n",
      "Época [4/6], Step [100/1875], Custo: 0.001, Acurácia: 100.0\n",
      "Época [4/6], Step [200/1875], Custo: 0.089, Acurácia: 96.875\n",
      "Época [4/6], Step [300/1875], Custo: 0.196, Acurácia: 96.875\n",
      "Época [4/6], Step [400/1875], Custo: 0.001, Acurácia: 100.0\n",
      "Época [4/6], Step [500/1875], Custo: 0.016, Acurácia: 100.0\n",
      "Época [4/6], Step [600/1875], Custo: 0.001, Acurácia: 100.0\n",
      "Época [4/6], Step [700/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [4/6], Step [800/1875], Custo: 0.022, Acurácia: 100.0\n",
      "Época [4/6], Step [900/1875], Custo: 0.001, Acurácia: 100.0\n",
      "Época [4/6], Step [1000/1875], Custo: 0.002, Acurácia: 100.0\n",
      "Época [4/6], Step [1100/1875], Custo: 0.11, Acurácia: 96.875\n",
      "Época [4/6], Step [1200/1875], Custo: 0.095, Acurácia: 96.875\n",
      "Época [4/6], Step [1300/1875], Custo: 0.071, Acurácia: 96.875\n",
      "Época [4/6], Step [1400/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [4/6], Step [1500/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [4/6], Step [1600/1875], Custo: 0.003, Acurácia: 100.0\n",
      "Época [4/6], Step [1700/1875], Custo: 0.002, Acurácia: 100.0\n",
      "Época [4/6], Step [1800/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [5/6], Step [100/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [5/6], Step [200/1875], Custo: 0.02, Acurácia: 100.0\n",
      "Época [5/6], Step [300/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [5/6], Step [400/1875], Custo: 0.026, Acurácia: 96.875\n",
      "Época [5/6], Step [500/1875], Custo: 0.154, Acurácia: 96.875\n",
      "Época [5/6], Step [600/1875], Custo: 0.001, Acurácia: 100.0\n",
      "Época [5/6], Step [700/1875], Custo: 0.124, Acurácia: 93.75\n",
      "Época [5/6], Step [800/1875], Custo: 0.024, Acurácia: 96.875\n",
      "Época [5/6], Step [900/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [5/6], Step [1000/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [5/6], Step [1100/1875], Custo: 0.025, Acurácia: 96.875\n",
      "Época [5/6], Step [1200/1875], Custo: 0.008, Acurácia: 100.0\n",
      "Época [5/6], Step [1300/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [5/6], Step [1400/1875], Custo: 0.002, Acurácia: 100.0\n",
      "Época [5/6], Step [1500/1875], Custo: 0.011, Acurácia: 100.0\n",
      "Época [5/6], Step [1600/1875], Custo: 0.001, Acurácia: 100.0\n",
      "Época [5/6], Step [1700/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [5/6], Step [1800/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [6/6], Step [100/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [6/6], Step [200/1875], Custo: 0.248, Acurácia: 96.875\n",
      "Época [6/6], Step [300/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [6/6], Step [400/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [6/6], Step [500/1875], Custo: 0.008, Acurácia: 100.0\n",
      "Época [6/6], Step [600/1875], Custo: 0.001, Acurácia: 100.0\n",
      "Época [6/6], Step [700/1875], Custo: 0.001, Acurácia: 100.0\n",
      "Época [6/6], Step [800/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [6/6], Step [900/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [6/6], Step [1000/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [6/6], Step [1100/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [6/6], Step [1200/1875], Custo: 0.005, Acurácia: 100.0\n",
      "Época [6/6], Step [1300/1875], Custo: 0.0, Acurácia: 100.0\n",
      "Época [6/6], Step [1400/1875], Custo: 0.001, Acurácia: 100.0\n",
      "Época [6/6], Step [1500/1875], Custo: 0.237, Acurácia: 90.625\n",
      "Época [6/6], Step [1600/1875], Custo: 0.007, Acurácia: 100.0\n",
      "Época [6/6], Step [1700/1875], Custo: 0.004, Acurácia: 100.0\n",
      "Época [6/6], Step [1800/1875], Custo: 0.26, Acurácia: 96.875\n"
     ]
    }
   ],
   "source": [
    "# Treinar o Modelo\n",
    "total_step = len(train_loader) # quantos batches eu tenho\n",
    "\n",
    "# Listas vazias\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Gera a propagação (feed forward)\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calcula a função-custo\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Retro-propagação (Backprop) e a otimização com Adam\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Acurácia\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Época [{epoch+1}/{epochs}], Step [{i+1}/{total_step}], Custo: {round(loss.item(), 3)}, Acurácia: {round((correct / total) * 100, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Acurácia do Modelo\n",
    "\n",
    "Usar o comando `model.eval()`\n",
    "\n",
    "Para a métrica acurácia, retorna um score de acurácia `float` entre $0$ e $1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do Modelo em 10k imagens de teste: 98.91\n"
     ]
    }
   ],
   "source": [
    "model.eval() # coloca o modelo em modo de avaliação (sem calcular gradientes)\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        # Feed-forward com as imagens de teste\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # gera predições usando a função max()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Acumula total e corretas\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print(f\"Acurácia do Modelo em 10k imagens de teste: {round((correct / total) * 100, 3)}\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "progress": true,
   "scroll": true,
   "slideNumber": true,
   "theme": "black"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
